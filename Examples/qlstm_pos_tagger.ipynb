{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cf027c2-6be1-4e57-babb-3607ab310b55",
   "metadata": {},
   "source": [
    "# Quantum LSTM for part-of-speech tagging\n",
    "\n",
    "In this example we will train a Quantum Long Short-Term Memory model to predict word types in sentences.\n",
    "\n",
    "Let us start by importing some libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a30b393",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append('../src')\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from IPython.display import Image\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "cmap = matplotlib.colormaps.get_cmap('tab20c')\n",
    "colors = cmap.colors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0bce13",
   "metadata": {},
   "source": [
    "For this task, the predicted label for the *t*-th word in a sentence is extracted from the hidden state in the memory block *t*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db32f93b-103b-4908-98f8-e7d2451f723f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='../Figures/lstmblocks.png', width=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536119ae-93f5-4587-9107-0ce226c43dac",
   "metadata": {},
   "source": [
    "### Reading the dataset\n",
    "\n",
    "The goal of the trained QLSTM model is to predict the labels for the words 20 of sentences in the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96704f34-98ae-4a44-b5b3-5e35e9da1b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from discocat_aux import read_data\n",
    "\n",
    "train_labels, train_data = read_data('../Datasets/mc_train_data.txt')\n",
    "val_labels, val_data = read_data('../Datasets/mc_test_data.txt')\n",
    "test_labels, test_data = read_data('../Datasets/mc_test_data.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7502119c-e25e-49c1-be79-20aad98d1dff",
   "metadata": {},
   "source": [
    "For this task, the label of a sentence is a list with indexes specifying its word types.\n",
    "\n",
    "To assign labels to the sentences in the dataset, we build a dictionary assigning a type to each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff74d10a-bb63-43b0-8ece-a19e36eef4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_idx = {'N': 0, 'V': 1, 'Adj': 2}\n",
    "idx_type = {idx: tag for tag, idx in type_idx.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f1f9c9",
   "metadata": {},
   "source": [
    "The type of each word in a sentence can be extracted from the sentence's diagram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886e8fb8-4b81-4c27-b083-05a938d00c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lambeq import BobcatParser\n",
    "\n",
    "parser = BobcatParser(root_cats=('S', 'N'), verbose='suppress')\n",
    "raw_train_diagrams = parser.sentences2diagrams(train_data, suppress_exceptions=True)\n",
    "raw_val_diagrams = parser.sentences2diagrams(val_data, suppress_exceptions=True)\n",
    "raw_test_diagrams = parser.sentences2diagrams(test_data, suppress_exceptions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c81cbcb-cbad-416d-8092-2cfc96dbe7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_diagrams = []\n",
    "val_diagrams = []\n",
    "test_diagrams = []\n",
    "\n",
    "for i, diagram in enumerate(raw_train_diagrams):\n",
    "    if diagram is not None: train_diagrams.append(diagram.normal_form())\n",
    "for i, diagram in enumerate(raw_val_diagrams):\n",
    "    if diagram is not None: val_diagrams.append(diagram.normal_form())\n",
    "for i, diagram in enumerate(raw_test_diagrams):\n",
    "    if diagram is not None: test_diagrams.append(diagram.normal_form())\n",
    "\n",
    "space_type = {'n': 'N', 'nsn': 'V', 'nn': 'Adj'}\n",
    "word_type = {}\n",
    "\n",
    "for i, diagram in enumerate(train_diagrams+val_diagrams+test_diagrams):\n",
    "    for j, word in enumerate(diagram.to_tree()['inside']):\n",
    "        if 'name' in word['inside'][1].keys():\n",
    "            if word['inside'][1]['name'] in word_type.keys(): continue\n",
    "            else:\n",
    "                for k, space in enumerate(word['inside'][1]['cod']['inside']):\n",
    "                    if k == 0: wtype = space['name']\n",
    "                    else: wtype += space['name']\n",
    "                word_type[word['inside'][1]['name']] = space_type[wtype]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9d4a06-41c5-45b1-8a21-eb63631c40ea",
   "metadata": {},
   "source": [
    "The dictionary for the dataset is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b3bd73-3d8b-4111-914f-628739de485a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (word, type) in word_type.items():\n",
    "    print('{}: {}'.format(word, type))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f424acb-4805-49e8-aa0c-ff25d538edc9",
   "metadata": {},
   "source": [
    "We then proceed to assing the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b64145-3bf9-4549-8f00-65d8afa07033",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = []\n",
    "validation_data = []\n",
    "testing_data = []\n",
    "\n",
    "for i, data in enumerate(train_data):\n",
    "    sentence = []\n",
    "    tags = []\n",
    "    for word in data.split():\n",
    "        try:\n",
    "            tags.append(word_type[word])\n",
    "            sentence.append(word)\n",
    "        except: continue\n",
    "    training_data.append((sentence, tags))\n",
    "\n",
    "for i, data in enumerate(val_data):\n",
    "    sentence = []\n",
    "    tags = []\n",
    "    for word in data.split():\n",
    "        try:\n",
    "            tags.append(word_type[word])\n",
    "            sentence.append(word)\n",
    "        except: continue\n",
    "    validation_data.append((sentence, tags))\n",
    "\n",
    "for i, data in enumerate(test_data):\n",
    "    sentence = []\n",
    "    tags = []\n",
    "    for word in data.split():\n",
    "        try:\n",
    "            tags.append(word_type[word])\n",
    "            sentence.append(word)\n",
    "        except: continue\n",
    "    testing_data.append((sentence, tags))\n",
    "\n",
    "word_idx = {}\n",
    "\n",
    "for sentence, tags in training_data+validation_data+testing_data:\n",
    "    for word in sentence:\n",
    "        if word not in word_idx: # word has not been assigned an index yet\n",
    "            word_idx[word] = len(word_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5263ed99-8127-4b76-964e-353ab313d11c",
   "metadata": {},
   "source": [
    "The QLSTM model for part-of-speech tagging is defined in *qlstm_models.py*. It was build on top of the QLSTM model at https://github.com/rdisipio/qlstm . We train it using *PyTorch*, the negative log likelihood loss function and the Adam gradient descent optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bbddcc-3d93-464c-89c4-14fd8beff155",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qlstm_models import QLSTMTagger\n",
    "\n",
    "embedding_dim = 8 # Dimension of the vector representing the word\n",
    "hidden_dim = 6 # \n",
    "n_qubits = 2\n",
    "backend = 'default.qubit'\n",
    "\n",
    "model = QLSTMTagger(embedding_dim,\n",
    "                    hidden_dim,\n",
    "                    vocab_size=len(word_idx),\n",
    "                    tagset_size=len(type_idx),\n",
    "                    n_qubits=n_qubits,\n",
    "                    backend=backend)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e45060-9ccf-4a2d-9486-baf049bc5001",
   "metadata": {},
   "source": [
    "Unlike the *QLSTMClassifier* class, which calculates the sentence's tag score from the last hidden state as shown below,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2aca79-6e29-4438-a479-4903955dfc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='../Figures/qlstmclassifier.png', width=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0ce89f-f2b8-48b0-9458-7700b01d2255",
   "metadata": {},
   "source": [
    "the *QLSTMTagger* class in *qlstm_models.py* calculates the sentence's tag score from the hidden states in every memory block, allowing us to assign a tag to each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc47543-4f4f-4eab-bb18-baf0495bf624",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='../Figures/qlstmtagger.png', width=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6258c0b-fb83-4e59-b5c5-5d299c98c78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e568b31-158b-47e5-b188-92e249e337b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "training = {'loss': [], 'acc': []}\n",
    "n_epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c27343-99ee-4e17-b528-713b7b7a2dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import mean, around\n",
    "from numpy.random import shuffle\n",
    "from qlstm_aux import prepare_sequence\n",
    "'''\n",
    "# To parallelize the model training, we split the training dataset among multiple MPI processes.\n",
    "# The gradient of the loss functrion are communicated before updating the parameters.\n",
    "\n",
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "jobs_per_rank = len(train_data) // size\n",
    "leftover = len(train_data) % size\n",
    "if rank > size-leftover-1: jobs_per_rank += 1\n",
    "jobsizes = comm.allgather(jobs_per_rank)\n",
    "starts = list(sum(jobsizes[:i]) for i in range(len(jobsizes)))\n",
    "rank_idxs = list(train_idxs[i] for i in range(starts[rank], starts[rank] + jobsizes[rank]))\n",
    "\n",
    "# Parallel version would require to replace train_idxs below for rank_idxs\n",
    "'''\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    rank_losses = []\n",
    "    rank_preds = []\n",
    "    rank_targets = []\n",
    "    accuracy = torch.tensor([0], dtype=torch.float32)\n",
    "    \n",
    "    for sentence, tags in training_data:\n",
    "        \n",
    "        model.zero_grad()\n",
    "\n",
    "        sentence_in = prepare_sequence(sentence, word_idx)\n",
    "        labels = prepare_sequence(tags, type_idx)\n",
    "\n",
    "        tag_scores = model(sentence_in)\n",
    "\n",
    "        loss = loss_function(tag_scores, labels)\n",
    "        loss.backward()\n",
    "        '''\n",
    "        for param in model.parameters():\n",
    "            temp = torch.tensor(zeros(param.shape), dtype=torch.float32)\n",
    "            comm.Allreduce(param.grad, temp, op=MPI.SUM)\n",
    "            param.grad = temp\n",
    "\n",
    "        if num_qubits > 0:\n",
    "            for param in model.lstm.VQC['forget'].parameters():\n",
    "                temp = torch.tensor(zeros(param.shape), dtype=torch.float32)\n",
    "                comm.Allreduce(param.grad, temp, op=MPI.SUM)\n",
    "                param.grad = temp\n",
    "            for param in model.lstm.VQC['input'].parameters():\n",
    "                temp = torch.tensor(zeros(param.shape), dtype=torch.float32)\n",
    "                comm.Allreduce(param.grad, temp, op=MPI.SUM)\n",
    "                param.grad = temp\n",
    "            for param in model.lstm.VQC['update'].parameters():\n",
    "                temp = torch.tensor(zeros(param.shape), dtype=torch.float32)\n",
    "                comm.Allreduce(param.grad, temp, op=MPI.SUM)\n",
    "                param.grad = temp\n",
    "            for param in model.lstm.VQC['output'].parameters():\n",
    "                temp = torch.tensor(zeros(param.shape), dtype=torch.float32)\n",
    "                comm.Allreduce(param.grad, temp, op=MPI.SUM)\n",
    "                param.grad = temp\n",
    "        '''\n",
    "        optimizer.step()\n",
    "        \n",
    "        rank_losses.append(float(loss))\n",
    "\n",
    "        probs = torch.softmax(tag_scores, dim=-1)\n",
    "        rank_preds.append(probs.argmax(dim=-1))\n",
    "        rank_targets.append(labels)\n",
    "    '''\n",
    "    losses = []\n",
    "    preds = []\n",
    "    targets = []\n",
    "\n",
    "    temp = comm.allgather(rank_losses)\n",
    "    for sublist in temp: losses += list(item for item in sublist)\n",
    "    temp = comm.allgather(rank_preds)\n",
    "    for sublist in temp: preds += list(item for item in sublist)\n",
    "    temp = comm.allgather(rank_targets)\n",
    "    for sublist in temp: targets += list(item for item in sublist)\n",
    "    if epoch == num_epochs-1:\n",
    "        temp = comm.allgather(rank_confusion)\n",
    "        confusion = {c: [] for c in classes}\n",
    "        for sublist in temp:\n",
    "            for c in classes: confusion[c] += sublist[c]\n",
    "    '''\n",
    "    avg_loss = mean(rank_losses)\n",
    "    training['loss'].append(avg_loss)\n",
    "    \n",
    "    preds = torch.cat(rank_preds)\n",
    "    targets = torch.cat(rank_targets)\n",
    "    corrects = (preds == targets)\n",
    "    accuracy = corrects.sum().float() / float(targets.size(0) )\n",
    "    training['acc'].append(accuracy.detach().numpy())\n",
    "\n",
    "    if epoch%10==0: print('Epoch: {}\\tLoss: {}\\tAccuracy: {}'.format(epoch, around(avg_loss,4), around(accuracy,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6362a8b8-3d10-4963-a2bb-69f1ff7996bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=2, sharex=True, figsize=(3.375*3/2, 3.375*4/3), dpi=200)\n",
    "for ax in axs.ravel():\n",
    "    ax.tick_params(labelsize='xx-small')\n",
    "axs[0].set_ylabel('Loss', fontsize='x-small')\n",
    "axs[1].set_ylabel('Accuracy', fontsize='x-small')\n",
    "axs[1].set_xlabel('Epoch', fontsize='x-small')\n",
    "axs[0].plot(training['loss'], lw=1, color=colors[1], label='Q'*(n_qubits>0)+'LSTM');\n",
    "axs[1].plot(training['acc'], lw=1, color=colors[1], label='Q'*(n_qubits>0)+'LSTM');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0576965-aa35-4270-9637-d82bfa7a54f5",
   "metadata": {},
   "source": [
    "Below the model is used to predict the part-of-speech tags on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396fe9cb-388f-457d-8741-bccc43b676e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 4\n",
    "for word in testing_data[test][0]:\n",
    "    print(word, end=' ')\n",
    "print('\\n')\n",
    "print('\\t\\t\\t', idx_type)\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = prepare_sequence(testing_data[test][0], word_idx)\n",
    "    tag_scores = model(inputs)\n",
    "\n",
    "    for i, tag in enumerate(torch.argmax(tag_scores,dim=-1).detach().numpy()):\n",
    "        print('{}: {}\\t\\t{}'.format(testing_data[test][0][i], idx_type[tag], tag_scores[i].detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c50442-b905-473f-ba12-0882899a57a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qnlp-tut",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
